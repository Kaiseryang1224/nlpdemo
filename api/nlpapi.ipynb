{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "print(doc.text)\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animal_component(doc):\n",
    "    animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    span = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    doc.ents = span\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global nlp\n",
    "nlp.add_pipe(animal_component, after=False)\n",
    "\n",
    "time_spent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def timer(func):\n",
    "    def f(*args, **kwargs):\n",
    "        global time_spent\n",
    "        before = time()\n",
    "        rv = func(*args, **kwargs)\n",
    "        after = time()\n",
    "        time_spent[func.__name__] = format(after - before, '.2f')# round(int(after - before), 3)\n",
    "        return rv\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_entities(doc):\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "@timer\n",
    "def get_verbs(doc):\n",
    "    return [(token.text, token.pos_) for token in doc if token.pos_ == \"VERB\" or token.pos_ == \"AUX\"]\n",
    "\n",
    "@timer\n",
    "def get_countries(doc):\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE']\n",
    "\n",
    "@timer\n",
    "def get_persons(doc):\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'PERSON']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBS   : [(\"dont't\", 'VERB'), ('do', 'AUX')]\n",
      "VERBS   : []\n",
      "VERBS   : []\n",
      "VERBS   : []\n",
      "{'get_verbs': '0.00', 'get_entities': '0.00', 'get_countries': '0.00', 'get_persons': '0.00'}\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "I dont't wanna do it today, maybe tomorrow morning.\n",
    "\"\"\"\n",
    "doc = nlp(TEXT)\n",
    "\n",
    "\n",
    "print('VERBS   :',  get_verbs(doc))\n",
    "print('VERBS   :',  get_entities(doc))\n",
    "print('VERBS   :',  get_countries(doc))\n",
    "print('VERBS   :',  get_persons(doc))\n",
    "print(time_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library\", \"We hope you don't hate it.\"])\n",
    "\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drug Runners and  a U.S. Senator have something to do with the Murder http://www.amazon.com/Circumstantial-Evidence-Getting-Florida-Bozarth-ebook/dp/B004FPZ452/ref=pd_rhf_p_t_1 The State Attorney Knows... NOW So Will You. GET Ypur Copy TODAY'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "p = '../dataset/datasets_73978_166769_fb_sentiment.csv'\n",
    "tweets = []\n",
    "with open(p, newline='') as f:\n",
    "    spamreader = csv.DictReader(f)\n",
    "    for row in spamreader:\n",
    "        tweets.append(row[\"FBPost\"])\n",
    "        \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres a single, to add, to Kindle. Just read this 19th century story: \"The Ghost of Round Island\". Its about a man (French/American Indian) and his dog sled transporting a woman across the ice, from Mackinac Island to Cheboygan - and the ghost that...\n",
      "Entities:    [('this 19th century', 'DATE'), ('The Ghost of Round Island', 'WORK_OF_ART'), ('French', 'NORP'), ('Mackinac Island', 'GPE'), ('Cheboygan', 'GPE')]\n",
      "Countries   : [('Mackinac Island', 'GPE'), ('Cheboygan', 'GPE')]\n",
      "PERSON   : []\n",
      "RESULT:   [{'label': 'NEGATIVE', 'score': 0.9923949837684631}]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(tweets[1])\n",
    "print(doc)\n",
    "print('Entities:   ', get_entities(doc))\n",
    "print('Countries   :',  get_countries(doc))\n",
    "print('PERSON   :',  get_persons(doc))\n",
    "\n",
    "results = classifier(\" you arent missing a thing, I tried this game and it sucks! \")\n",
    "print(\"RESULT:  \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanick/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6225793957710266, 'start': 34, 'end': 96, 'answer': 'the task of extracting an answer from a text given a question.'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "\n",
    "nlp = pipeline(\"question-answering\")\n",
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ad60e9d6954bbfa2616a1fbdc07002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=998, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d32924494b44cf39e3f72286f68e659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379ce66e62904211840edb5ecb799f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=60, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594e0e7dd3a645e39fab7e6f3927e0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2187efe9304dd98e8f254c0456a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1334448817, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002 . At one time, she was married to eight men at once, prosecutors say .\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "\n",
    "s = summarizer(ARTICLE, max_length=118, min_length=30, do_sample=False)\n",
    "print(s[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "[{'summary_text': ' In order to analyze what has been done in Cape Verde with regard to Intellectual Property and Digital Signature, we interviewed the competent entities related to each particular subject. We conducted interviews and applied questionnaires that we sent through electronic mails and social networks to our artists and authors .'}]\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = \"\"\"\n",
    "In order to analyze what has been done in Cape Verde with regard to Intellectual Property \n",
    "and also Digital Signature, we interviewed the competent entities related to each particular subject\n",
    "and conducted interviews and applied questionnaires that we sent through electronic mails \n",
    "and social networks to our artists and authors both in Cape Verde and some who are living\n",
    "abroad. With the data collected it was possible to verify that the protection of Intellectual\n",
    "Property is still in a phase of maturation in our archipelago and having been given more\n",
    "attention in recent years.\n",
    "\"\"\"\n",
    "print(len(ARTICLE))\n",
    "\n",
    "print(summarizer(ARTICLE, max_length=118, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entities:    [('Maria', 'PERSON'), ('Kindle', 'PERSON'), ('a few hours', 'TIME')]\n",
      "Countries   : []\n",
      "PERSON   : [('Maria', 'PERSON'), ('Kindle', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "s = \"@Maria: Do you mean the Nook? Be careful, books you buy for the Kindle are for that piece of electronics, and vice versa. I love my Kindle, there are people that swear by the Nook. They like the color screen.Me? I want an ereader that is a reader-- so I dont need color. The kindle battery lasts longer, and the unit isnt as heavy, which can make a difference after reading for a few hours. :) \" # Sample string \n",
    "out = s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "doc = nlp(s.replace(\"@\", \"\"))\n",
    "print()\n",
    "print('Entities:   ', get_entities(doc))\n",
    "print('Countries   :',  get_countries(doc))\n",
    "print('PERSON   :',  get_persons(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(\"This is a sentence\")\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
